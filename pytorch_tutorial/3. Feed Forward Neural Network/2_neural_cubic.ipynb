{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Cubic Data (multi-dimensional)\n",
    "- y = x^3 -3x^2 -9x -1\n",
    "- 5 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from visdom import Visdom\n",
    "viz = Visdom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lim/anaconda3/envs/NLPApps/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/lim/anaconda3/envs/NLPApps/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "num_data = 1000\n",
    "num_epoch = 5000\n",
    "\n",
    "x = init.uniform(torch.Tensor(num_data,1),-10,10)\n",
    "y = (x**3) - 3*(x**2) - 9*x - 1\n",
    "\n",
    "noise = init.normal(torch.FloatTensor(num_data,1),std=3)\n",
    "\n",
    "y_noise = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "\n",
    "input_data = torch.cat([x,y_noise],1)\n",
    "\n",
    "win=viz.scatter(\n",
    "    X = input_data,\n",
    "    opts=dict(\n",
    "        xtickmin=-10,\n",
    "        xtickmax=10,\n",
    "        xtickstep=1,\n",
    "        ytickmin=0,\n",
    "        ytickmax=500,\n",
    "        ytickstep=1,\n",
    "        markersymbol='dot',\n",
    "        markercolor=np.random.randint(0, 255, num_data),\n",
    "        markersize=5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "\n",
    "input_data = torch.cat([x,y_noise],1)\n",
    "\n",
    "win=viz.scatter(\n",
    "    X = torch.cat([x,y],1),\n",
    "    opts=dict(\n",
    "        xtickmin=-10,\n",
    "        xtickmax=10,\n",
    "        xtickstep=1,\n",
    "        ytickmin=0,\n",
    "        ytickmax=500,\n",
    "        ytickstep=1,\n",
    "        markersymbol='dot',\n",
    "        markercolor=np.random.randint(0, 255, num_data),\n",
    "        markersize=5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(1,20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20,10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,1),\n",
    "        )\n",
    "\n",
    "loss_func = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(208.7360, grad_fn=<L1LossBackward>)\n",
      "tensor(208.4341, grad_fn=<L1LossBackward>)\n",
      "tensor(207.6888, grad_fn=<L1LossBackward>)\n",
      "tensor(204.9050, grad_fn=<L1LossBackward>)\n",
      "tensor(179.3635, grad_fn=<L1LossBackward>)\n",
      "tensor(131.9337, grad_fn=<L1LossBackward>)\n",
      "tensor(129.3835, grad_fn=<L1LossBackward>)\n",
      "tensor(126.4591, grad_fn=<L1LossBackward>)\n",
      "tensor(122.7979, grad_fn=<L1LossBackward>)\n",
      "tensor(117.9157, grad_fn=<L1LossBackward>)\n",
      "tensor(109.7331, grad_fn=<L1LossBackward>)\n",
      "tensor(97.5420, grad_fn=<L1LossBackward>)\n",
      "tensor(83.6747, grad_fn=<L1LossBackward>)\n",
      "tensor(92.4683, grad_fn=<L1LossBackward>)\n",
      "tensor(95.2579, grad_fn=<L1LossBackward>)\n",
      "tensor(83.7875, grad_fn=<L1LossBackward>)\n",
      "tensor(90.0640, grad_fn=<L1LossBackward>)\n",
      "tensor(87.8785, grad_fn=<L1LossBackward>)\n",
      "tensor(80.0949, grad_fn=<L1LossBackward>)\n",
      "tensor(85.9687, grad_fn=<L1LossBackward>)\n",
      "tensor(78.0256, grad_fn=<L1LossBackward>)\n",
      "tensor(83.8751, grad_fn=<L1LossBackward>)\n",
      "tensor(79.4116, grad_fn=<L1LossBackward>)\n",
      "tensor(84.2814, grad_fn=<L1LossBackward>)\n",
      "tensor(82.4239, grad_fn=<L1LossBackward>)\n",
      "tensor(81.9011, grad_fn=<L1LossBackward>)\n",
      "tensor(70.8806, grad_fn=<L1LossBackward>)\n",
      "tensor(81.5888, grad_fn=<L1LossBackward>)\n",
      "tensor(80.8857, grad_fn=<L1LossBackward>)\n",
      "tensor(73.7076, grad_fn=<L1LossBackward>)\n",
      "tensor(68.8560, grad_fn=<L1LossBackward>)\n",
      "tensor(79.2748, grad_fn=<L1LossBackward>)\n",
      "tensor(79.1227, grad_fn=<L1LossBackward>)\n",
      "tensor(74.7291, grad_fn=<L1LossBackward>)\n",
      "tensor(68.6278, grad_fn=<L1LossBackward>)\n",
      "tensor(75.6615, grad_fn=<L1LossBackward>)\n",
      "tensor(72.2403, grad_fn=<L1LossBackward>)\n",
      "tensor(72.1848, grad_fn=<L1LossBackward>)\n",
      "tensor(71.3706, grad_fn=<L1LossBackward>)\n",
      "tensor(72.5332, grad_fn=<L1LossBackward>)\n",
      "tensor(72.4553, grad_fn=<L1LossBackward>)\n",
      "tensor(72.6134, grad_fn=<L1LossBackward>)\n",
      "tensor(73.5687, grad_fn=<L1LossBackward>)\n",
      "tensor(75.9927, grad_fn=<L1LossBackward>)\n",
      "tensor(71.9053, grad_fn=<L1LossBackward>)\n",
      "tensor(71.5686, grad_fn=<L1LossBackward>)\n",
      "tensor(75.1793, grad_fn=<L1LossBackward>)\n",
      "tensor(70.2687, grad_fn=<L1LossBackward>)\n",
      "tensor(70.8308, grad_fn=<L1LossBackward>)\n",
      "tensor(72.0387, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_arr =[]\n",
    "label = y_noise\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    output = model(x)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_func(output,label)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 ==0:\n",
    "        print(loss)\n",
    "        \n",
    "    #loss_arr.append(loss.cpu().data.numpy()[0])\n",
    "    loss_arr.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.4540],\n",
      "        [ 1.0252],\n",
      "        [-0.5678],\n",
      "        [ 0.2287],\n",
      "        [ 0.6094],\n",
      "        [-0.7664],\n",
      "        [-1.0896],\n",
      "        [-1.1309],\n",
      "        [ 0.6947],\n",
      "        [-0.3824],\n",
      "        [ 0.9449],\n",
      "        [ 0.7942],\n",
      "        [-1.0012],\n",
      "        [-0.6286],\n",
      "        [ 0.5754],\n",
      "        [-0.9163],\n",
      "        [ 0.0274],\n",
      "        [ 0.7330],\n",
      "        [-0.0401],\n",
      "        [-0.6313]], requires_grad=True), Parameter containing:\n",
      "tensor([-1.3936,  0.4838, -3.2044,  1.9319, -1.2221, -4.3261, -4.2213, -2.3490,\n",
      "         0.3218, -2.1195,  0.1188, -0.0459, -1.4427, -0.5137, -1.9262, -5.1691,\n",
      "        -0.2698,  0.6586, -0.9154, -2.4413], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1854,  0.1256,  0.2825, -0.2115,  0.1148,  0.1050,  0.2049,  0.2290,\n",
      "          0.0255,  0.0030, -0.0127, -0.0342,  0.4097,  0.3419, -0.1010,  0.1530,\n",
      "          0.0666, -0.0884,  0.0188,  0.0036],\n",
      "        [ 0.1388,  0.1148,  0.8004, -0.7498, -0.0662,  1.0421,  0.7909,  0.3913,\n",
      "          0.0959,  0.2396, -0.0750,  0.0305, -0.0590, -0.2878,  0.0613,  1.2454,\n",
      "          0.1608, -0.0801,  0.1184,  0.2930],\n",
      "        [-0.1566, -0.1162,  1.1757, -0.5067, -0.0503,  1.5971,  1.2873,  0.9239,\n",
      "          0.0586,  0.8848,  0.0870, -0.1088,  0.6999,  0.4126,  0.0648,  1.8957,\n",
      "          0.1157,  0.0668,  0.1753,  0.7638],\n",
      "        [-0.1095,  0.0844,  0.0931, -0.1722,  0.1208, -0.2213,  0.1567,  0.1651,\n",
      "         -0.1313,  0.1789, -0.0271, -0.1984, -0.0642, -0.0867, -0.1906, -0.1450,\n",
      "          0.0386, -0.1734, -0.0999,  0.1591],\n",
      "        [ 0.4827,  0.2200, -0.1648, -0.1087,  0.3769, -0.1183,  0.0913,  0.1929,\n",
      "          0.3998, -0.1294,  0.2165,  0.4071, -0.1077, -0.2007,  0.1325,  0.0492,\n",
      "         -0.0984,  0.3700, -0.1721,  0.1832],\n",
      "        [-0.1793,  0.1690, -0.0232, -0.2567, -0.2089,  0.1421, -0.0863,  0.0115,\n",
      "         -0.0237, -0.1423, -0.1713,  0.0715,  0.2023,  0.0096, -0.2078,  0.0871,\n",
      "          0.2047, -0.2100, -0.0566, -0.0239],\n",
      "        [ 0.0977, -0.0148, -0.1364,  0.1559,  0.1127,  0.1603, -0.1419, -0.1156,\n",
      "         -0.1767, -0.0903, -0.1025, -0.1165,  0.1981,  0.0278,  0.1516, -0.1927,\n",
      "         -0.0797, -0.0334,  0.1252,  0.0579],\n",
      "        [ 0.1646,  0.3740, -0.0487,  0.1614,  0.4836, -0.1635, -0.1606,  0.0427,\n",
      "          0.1996, -0.2309,  0.1396,  0.3852,  0.1560,  0.2523,  0.5816, -0.1166,\n",
      "         -0.2191,  0.2942,  0.1601, -0.2015],\n",
      "        [-1.1658,  0.6562,  0.5635,  0.2017, -0.9609,  0.7826,  0.5085,  0.4131,\n",
      "          0.3459,  0.3308,  0.2611,  0.0571,  0.5804,  0.4803, -1.6788,  0.9136,\n",
      "          0.0194,  0.2834, -0.0870,  0.3323],\n",
      "        [-0.2014,  0.1664,  0.2096,  0.0033,  0.0091,  0.3480, -0.0063,  0.1124,\n",
      "         -0.2198, -0.0664,  0.0124,  0.1229,  0.2352,  0.3069,  0.0873,  0.3374,\n",
      "         -0.0859, -0.1400, -0.1879,  0.2471]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3281, -0.9009, -1.4885, -0.1214, -0.1838, -0.2725, -0.1015,  0.1931,\n",
      "        -0.2954, -0.0538], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2510,  0.4938,  1.2165,  0.2553,  0.0066,  0.0648, -0.2597, -0.4216,\n",
      "          1.1336,  0.1516],\n",
      "        [ 0.4909,  1.6512,  2.5621, -0.2695, -0.2791,  0.2589,  0.2703, -0.2743,\n",
      "          1.9800,  0.5322],\n",
      "        [-0.0783, -0.3081, -0.2905, -0.2363,  0.7948, -0.2010,  0.0075,  0.8835,\n",
      "         -0.2009,  0.2342],\n",
      "        [ 0.2179,  0.9645,  1.5069,  0.1815, -0.3532, -0.0510,  0.1328, -0.0621,\n",
      "          1.2477,  0.1084],\n",
      "        [ 0.2993, -0.1374, -0.1564,  0.2503, -0.1084, -0.1110, -0.2170,  0.1050,\n",
      "          0.0687,  0.2939]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0963, -0.5082,  0.1497, -0.1322, -0.2406], requires_grad=True), Parameter containing:\n",
      "tensor([[-1.7205, -3.7275,  1.1838, -2.2284, -0.1249]], requires_grad=True), Parameter containing:\n",
      "tensor([0.9878], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "param_list = list(model.parameters())\n",
    "print(param_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Trained Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "win2=viz.scatter(\n",
    "    X = torch.cat([x, output],1),\n",
    "    opts=dict(\n",
    "        xtickmin=-10,\n",
    "        xtickmax=10,\n",
    "        xtickstep=1,\n",
    "        ytickmin=0,\n",
    "        ytickmax=500,\n",
    "        ytickstep=1,\n",
    "        markersymbol='dot',\n",
    "        markercolor=np.random.randint(0, 255, num_data),\n",
    "        markersize=5,\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape([i for i in range(num_epoch)],newshape=[num_epoch,1])\n",
    "loss_data = np.reshape(loss_arr,newshape=[num_epoch,1])\n",
    "\n",
    "win3=viz.line(\n",
    "    X = x,\n",
    "    Y = loss_data, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
